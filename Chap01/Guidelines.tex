%Formatting Guidelines for Writing Dissertation.
\chapter{Introduction}\label{Guidelines}
\section{Introduction}
Misinformation spreads quickly on social media which often uses both text and images, that makes detection a multimodal challenge. Traditional approaches usually handle text and images separately or combine them in a basic, straightforward manner. This weak integration results in poor semantic alignment between modalities that struggles in cases where text and image contradict each other, and offers little transparency into why a prediction was made. Models such as SpotFake use BERT for text and VGG19 for images with equal-weight feature fusion, but this approach offers limited interaction between the modalities and often faces trouble to generalize to complex, real-world misinformation.

Recent advances in multimodal learning provide more effective solutions. First, \textbf{contrastive learning} (in a CLIP-style setup) aligns text and image embeddings by bringing related pairs closer and pushing unrelated pairs apart thus improving cross-modal understanding before supervised training. Second, \textbf{cross-modal attention} allows the model to selectively focus on the most informative modality or token for each post. Finally, \textbf{explainability techniques} such as Grad-CAM for images and token-level SHAP for text helps in finding the reasoning behind predictions, enhancing transparency and supporting more thorough error analysis. Additionally, replacing VGG19 with ResNet50 strengthens visual encoding by offering better representations and inductive biases.

This project integrates these advancements into a single, end-to-end pipeline for fake news detection on Twitter posts. The system employs BERT for textual encoding, ResNet50 for image encoding (with comparative analysis against VGG19), contrastive pre-training for cross-modal alignment, cross-modal attention for intelligent fusion, and explainability techniques to make the decision-making process interpretable.

\section{Objectives}
The primary objectives of this project are:
\begin{itemize}
    \item To design a multimodal architecture combining BERT and ResNet50 (and compare against VGG19) for robust text–image encoding.
    \item To pre-train the text and image encoders using CLIP-style contrastive learning thus improving the alignment between modalities and enhancing overall performance on downstream tasks.
    \item To introduce cross-modal attention for observation-wise and selective fusion instead of simple concatenation.
    \item To integrate explainability methods such as Grad-CAM (for image regions), SHAP (for token-level importance), and attention heatmaps (for text–image focus) to make predictions transparent.
\end{itemize}

\section{Organization of Project}
This project is structured into six chapters and each chapter is built based on the previous to present a complete picture of the research process:

\begin{itemize}
    \item \textbf{Chapter 1} introduces the problem of multimodal fake news detection, presents the motivation for this research, and defines the project objectives.
    \item \textbf{Chapter 2} reviews the related work on multimodal misinformation detection, contrastive pre-training, attention-based fusion, and explainability in vision–language models.
    \item \textbf{Chapter 3} details the proposed methodology that includes encoder selection (VGG19 vs ResNet50).
    \item \textbf{Chapter 4} discusses the summary of the current approach and future work such as incorporating contrastive pre-training, cross-modal attention, and further explainability techniques and lastly concludes the project.
\end{itemize}

\chapter{Literature Review}
The spread of false information on social media has changed from simple text-based messages to more complex posts that include text, claims, and images. In the late, methods for detecting abbreviation mainly treated it as a text-focused problem, using word choices, writing styles, and how information spreads to classify false content~\cite{kwon2017rumor}. These techniques, which included simple word lists with basic models, recurrent and convolutional neural networks, and eventually transformer models, worked well with text-heavy datasets but struggled when images supported or added to the false claim. On the image side, tools that only used images, like those based on hand-coded features or CNN models like VGG and ResNet, could spot obvious changes or simple tricks in images~\cite{zhou2017learning}. But they had a hard time matching the meaning of the image with the text that accompanied it.

As datasets that include both text and images started to appear (such as Twitter and Weibo rumors, FakeNewsNet, Fakeddit, and MM-COVID), study moved toward models that could feel at both text and images.
The original type of these models used late fusion, where each part (text and images) was processed separately and then joined together before making a decision. This approach performed better than previous methods and was easy to use and fast. However, late fusion treated each part equally and couldn't handle situations where the two parts conflicted or differ. It also tended to favor whichever type of data was more useful in the training data.

To fix these issues, attention-based methods and cross-modal reasoning started to be used more.
Techniques like co-attention, bilinear pooling, and transformer-style cross-attention let models weigh diverse parts of the data selectively, emphasize the most important text and image elements. Vision-language transformers like VisualBERT~\cite{li2019visualbert}, ViLBERT, and LXMERT~\cite{tan2019lxmert} showed that having a shared understanding of both text and image data outperforms just joining them directly, especially when the linkage between text and picture is important for judging truthfulness. These models similarly made it easier to question how they make decisions, leading to further explainable approach for checking misinformation.

At the similar moment, contrastive pre-training greatly enhance how well text and images match.
Training methods like CLIP use a loss function called InfoNCE to bring similar text and image pairs closer together and push different ones apart in a shared space. This creates strong encoders that work well for both text and image tasks, making it easier to apply them to new problems without much training. When used for detecting misinformation, this method helps reduce reliance on misleading patterns that only work in one type of data and makes the system more robust when dealing with new or different types of information. Even small amounts of training with similar data have been shown to help improve accuracy and fairness. Alignment measures like Recall@K and mean reciprocal rank also relate to how well these models perform in real-world tasks.

Explainability has turn key for faith use.
For images, methods like Grad-CAM and Grad-CAM++ show which parts of the image are important without needing additional training. For text, tools like SHAP or Integrated Gradients help distinguish which words most influence the decision. Attention maps from multimodal models likewise help by showing how different parts of the text and image relate, though they don't clarify everything on their own. Together, these tools allow for quality checks, disclose misleading patterns like over-reliance on certain words, and support human moderation by provide clear reasons for opinion.

Despite these promotion, challenges remain.
Many systems still just touch text and images without properly aligning them, which can determinant problems when the data conflicts or when one is missing or noisy. Interpretability across both text and images is not widely used, and evaluating these models in terms of fairness, cross-topic performance, and handling new types of information is not as developed as measuring accuracy. Additionally, there's little comparison between different types of image models (like VGG19, ResNet50, and newer CNNs) within the same system under realistic computing conditions. These issues highlight the need for an integrated solution that includes:
\begin{itemize}
    \item Replacing VGG19 with ResNet50 for stronger image modeling
    \item Using CLIP-style contrastive learning to align text and image data
    \item Applying cross-modal attention for better combination of text and images
    \item Incorporating Grad-CAM and SHAP for ready-to-use explanations, all trained efficiently on multi-GPU systems
\end{itemize}

\begin{table}[!ht]
    \caption{Evolution of models for multimodal misinformation detection.} \vspace{0.2cm}
    \centering
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|p{3cm}|p{3.5cm}|p{2cm}|p{3.5cm}|p{4cm}|p{2cm}|}
        \hline
        \textbf{Era/Year} & \textbf{Representative Models} & \textbf{Modality} & \textbf{Fusion/Training Strategy} & \textbf{Key Contribution} & \textbf{Citations} \\ \hline
        Early 2010s (Text-only) & TF–IDF  SVM/LogReg; LSTM/GRU & Text & Unimodal supervised & Strong lexical baselines; limited visual reasoning & \cite{kwon2017rumor} \\ \hline
        2015–2018 (Vision-only) & VGG16/19, ResNet50 classifiers & Image & Unimodal supervised & Visual manipulation cues; limited to image evidence & \cite{zhou2017learning} \\ \hline
        2018–2021 (Late fusion) & CNN/RNN  VGG/ResNet  concat & Text+Image & Feature concatenation (late fusion) & First multimodal gains; simple and fast & \cite{singhal2019spotfake} \\ \hline
        2021 (Cross-modal Transformers) & VisualBERT, ViLBERT, LXMERT & Text+Image & Co-/cross-attention; joint contextualization & Learned alignment and selective fusion improve over concat & \cite{li2019visualbert} \\ \hline
        After 2022 (Advanced fusion) & MMBT, UNITER, OSCAR & Text+Image & Region-level features; transformer fusion & Finer grounding via object regions and captions & \cite{chen2020uniter} \\ \hline
    \end{tabular}
    }
    \label{tab:evolution_multimodal}
\end{table}

Table~\ref{tab:evolution_multimodal} summarizes the evolution of models for multimodal misinformation detection. It traces the progression from early unimodal approaches (text-only and vision-only) to modern integrated systems that leverage cross-modal transformers, contrastive learning, and explainability techniques. Each era brought specific contributions, from basic concatenation-based fusion to advanced fusion based mechanisms.


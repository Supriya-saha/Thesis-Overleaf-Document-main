%Formatting Guidelines for Writing Dissertation.
\chapter{Introduction}\label{Guidelines}
\section{Introduction}
Misinformation spreads quickly on social media which often uses both text and images, that makes detection a multimodal challenge. Traditional approaches usually handle text and images separately or combine them in a basic, straightforward manner. This weak integration results in poor semantic alignment between modalities that struggles in cases where text and image contradict each other, and offers little transparency into why a prediction was made. Models such as SpotFake use BERT for text and VGG19 for images with equal-weight feature fusion, but this approach offers limited interaction between the modalities and often faces trouble to generalize to complex, real-world misinformation.

Recent advances in multimodal learning provide more effective solutions. First, \textbf{contrastive learning} (in a CLIP-style setup) aligns text and image embeddings by bringing related pairs closer and pushing unrelated pairs apart thus improving cross-modal understanding before supervised training. Second, \textbf{cross-modal attention} allows the model to selectively focus on the most informative modality or token for each post. In​‍​‌‍​‍‌​‍​‌‍​‍‌ the end, \textbf{explainability techniques} like Grad-CAM for images and token-level SHAP for text assist in uncovering the logic of the predictions, thus, making the system more transparent and allowing for deeper error analysis. Moreover, the substitution of VGG19 with ResNet50 not only makes the visual encoding more powerful but also provides better representations and inductive biases.

This endeavor consolidates these innovations into one cohesive, end-to-end pipeline for the detection of false news in Twitter posts. The network comprises BERT for textual encoding, ResNet50 for image encoding (with a comparative study against VGG19), contrastive pre-training for cross-modal alignment, cross-modal attention for the efficient merging of information, and explainability techniques to illuminate the decision-making process.  

\section{Objectives}
The main objectives of this project are:
\begin{itemize}
    \item To design a multimodal architecture combining BERT and ResNet50 (and compare against VGG19) for robust text-image encoding.
    \item To pre-train the text and image encoders using CLIP-style contrastive learning thus improving the alignment between modalities and enhancing overall performance on downstream tasks.
    \item To introduce cross-modal attention for observation-wise and selective fusion instead of simple concatenation.
    \item To integrate explainability methods such as Grad-CAM (for image regions), SHAP (for token-level importance), and attention heatmaps (for text-image focus) to make predictions transparent.
\end{itemize}

\section{Organization of Project}
This project is structured into six chapters and each chapter is built based on the previous to present a complete picture of the research process:

\begin{itemize}
    \item \textbf{Chapter 1} introduces the problem of multimodal fake news detection, presents the motivation for this research, and defines the project objectives.
    \item \textbf{Chapter 2} reviews the related work on multimodal misinformation detection, contrastive pre-training, attention-based fusion, and explainability in vision–language models.
    \item \textbf{Chapter 3} details the proposed methodology that includes encoder selection (VGG19 vs ResNet50).
    \item \textbf{Chapter 4} discusses the summary of the current approach and future work such as incorporating contrastive pre-training, cross-modal attention, and further explainability techniques and lastly concludes the project.
\end{itemize}

\chapter{Literature Review}
The spread of false information on social media has changed from simple text-based messages to more complex posts that include text, claims, and images. In the late, methods for detecting abbreviation mainly treated it as a text-focused problem, using word choices, writing styles, and how information spreads to classify false content~\cite{kwon2017rumor}. These techniques, which included simple word lists with basic models, recurrent and convolutional neural networks, and eventually transformer models, worked well with text-heavy datasets but struggled when images supported or added to the false claim. On the image side, tools that only used images, like those based on hand-coded features or CNN models like VGG and ResNet, could spot obvious changes or simple tricks in images~\cite{zhou2017learning}. But they had a hard time matching the meaning of the image with the text that accompanied it.

When datasets containing both text and images came into existence (e.g., Twitter and Weibo rumors, FakeNewsNet, Fakeddit, and MM-COVID), research gravitated towards models that could process both text and images simultaneously. The initial version of these models employed late fusion, where each modality (text and images) was handled separately and then merged before making a decision. This method outstripped the performance of earlier techniques and was also user-friendly and quick. Nevertheless, late fusion treated each modality equally and was unable to cope with situations where the two modalities contradicted or differed. Besides, it also had a tendency to choose the modality that was more useful in the training data. To solve these problems, cell phone-based methods and cross-modal reasoning started to be more and more employed.
Models like co-attention, bilinear pooling, and transformer-style cross-attention enable models to selectively weigh different parts of the input, thereby allowing them to focus on the most relevant text and image components. Conceptualized under the same roof, the vision-language transformers viz. VisualBERT~\cite{li2019visualbert}, ViLBERT, and LXMERT~\cite{tan2019lxmert} established that combining the two modalities for a joint understanding rather than just concatenating them works better, particularly when the determination of truthfulness relies on the linkage between text and image. These make it equally simple to interrogate how they arrive at decisions, which in turn leads to more explainable methods for misinformation verification.
Simultaneously, contrastive pre-training has been instrumental in the remarkable improvement of the text and image matching capabilities. One such training method, CLIP, employs a loss function known as InfoNCE, which is used to bring similar text and image pairs closer together and different ones far apart in a joint space. The end products are powerful encoders that perform excellently on both text and image tasks, thereby facilitating effortless applications of these encoders to new problems with little or no further training. When this technique is utilized for the task of misinformation detection, it considerably mitigates the system's reliance on deceptive patterns that work only in one modality, thus rendering the system more resistant to handling novel or different types of information. Even minimal exposure to similar data has been proven to be beneficial in terms of both accuracy and fairness. Recall@K and mean reciprocal rank, along with other alignment metrics, are similarly linked to the real-world performance of these models.
Explainability has become a turning point for trustful usage. For images, methods like Grad-CAM visually demonstrate the areas of the image that are most relevant without requiring extra training. In the case of text, instruments such as SHAP helps in identifying the words that have the greatest impact on the decision. Attention maps from multimodal models also come in handy by indicating how different segments of the text and image correspond to each other, although they do not shed light on everything by themselves. Individually, these instruments make quality control feasible, uncover deceptive tactics such as illegitimate use of certain words, and assist human moderation through supplying clear and understandable account of the opinion granting process.
Challenges that these promotion efforts still have to confront remain. A lot of systems merely make a slight contact between text and images without actually aligning them and this can cause problems when data contradicts or one is missing or noisy. Interpretability over both text and images is hardly prevalent and judging these models in terms of fairness, performance across different topics, and ability to deal with novel types of information is not as mature as accuracy measurement. Moreover, there is also very little benchmarking of different image model types (like VGG19, ResNet50, and the latest CNNs) within the same setup under real-world working conditions. These issues highlight the need for an integrated solution that includes:
\begin{itemize}
    \item Replacing VGG19 with ResNet50 for stronger image modeling
    \item Using CLIP-style contrastive learning to align text and image data
    \item Applying cross-modal attention for better combination of text and images
    \item Incorporating Grad-CAM and SHAP for ready-to-use explanations, all trained efficiently on multi-GPU systems
\end{itemize}

\begin{table}[!ht]
    \caption{Evolution of models for multimodal misinformation detection.} \vspace{0.2cm}
    \centering
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{|p{3cm}|p{3.5cm}|p{2cm}|p{3.5cm}|p{4cm}|p{2cm}|}
        \hline
        \textbf{Era/Year} & \textbf{Representative Models} & \textbf{Modality} & \textbf{Fusion/Training Strategy} & \textbf{Key Contribution} & \textbf{Citations} \\ \hline
        Early 2010s (Text-only) & TF–IDF  SVM/LogReg; LSTM/GRU & Text & Unimodal supervised & Strong lexical baselines; limited visual reasoning & \cite{kwon2017rumor} \\ \hline
        2015–2018 (Vision-only) & VGG16/19, ResNet50 classifiers & Image & Unimodal supervised & Visual manipulation cues; limited to image evidence & \cite{zhou2017learning} \\ \hline
        2018–2021 (Late fusion) & CNN/RNN  VGG/ResNet  concat & Text+Image & Feature concatenation (late fusion) & First multimodal gains; simple and fast & \cite{singhal2019spotfake} \\ \hline
        2021 (Cross-modal Transformers) & VisualBERT, ViLBERT, LXMERT & Text+Image & Co-/cross-attention; joint contextualization & Learned alignment and selective fusion improve over concat & \cite{li2019visualbert} \\ \hline
        After 2022 (Advanced fusion) & MMBT, UNITER, OSCAR & Text+Image & Region-level features; transformer fusion & Finer grounding via object regions and captions & \cite{chen2020uniter} \\ \hline
    \end{tabular}
    }
    \label{tab:evolution_multimodal}
\end{table}

Table~\ref{tab:evolution_multimodal} summarizes the evolution of models for multimodal misinformation detection. It traces the progression from early unimodal approaches (text-only and vision-only) to modern integrated systems that leverage cross-modal transformers, contrastive learning, and explainability techniques. Each era brought specific contributions, from basic concatenation-based fusion to advanced fusion based mechanisms.


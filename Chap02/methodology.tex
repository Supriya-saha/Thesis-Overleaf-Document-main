\chapter{Methodology for Multimodal Fake News Detection} \label{Methodology} 

\section{Motivation}
Existing systems for detecting fake news that use both text and images still face many challenges. They struggle to properly connect information between the two types of data and to explain how their decisions are made. Most of these systems simply combine the features from text and images. This method assumes both text and image contribute equally, but that’s not always true. Sometimes, the text is more important; other times, the image carries more meaning. Because of this, such systems can easily fail when one of them is missing or misleading.
The type of image model used also plays a big role in performance. Older models like VGG19 work well but have some drawbacks. They produce very large feature vectors and can easily overfit, especially when the dataset is small. They also suffer from problems like vanishing gradients during training. On the other hand, ResNet models solve these issues by using skip connections, which make training smoother. ResNet50 also gives more compact and meaningful image features. Studies have shown that ResNet-based models perform better when the data changes across domains and can generalize well even with less labeled data.
The semantic gap difference is another major problem between text and image models how they learn. Text models like BERT learn from language patterns, while image models learn from visual details like colors and objects. When these are trained separately, their outputs do not correspond, thus it is difficult for a model to figure out the relationship between the text and the image. Contrastive learning solves this problem by teaching the model to make the distances between the matching text-image pairs smaller and the mismatched ones larger. Thus the system can now detect agreements as well as contradictions between the image and the text.
At last, explainability is a vital factor in the establishment of trust in such systems. In the case where a model predicts an outcome without indicating the reason, it becomes quite difficult to trust or enhance it. Instruments such as Grad-CAM (for indicating the most relevant image regions), SHAP (for pointing out the most significant words), and attention maps (for demonstrating how text and image interact) help to disclose the model's reasoning.


Figure \ref{fig1} presents the proposed multimodal fake news detection architecture and Table \ref{tab:dataset_summary} summarizes the datasets used in this study.

\begin{table}[!ht]
    \caption{Summary of Datasets Used for Multimodal Fake News Detection.}
    \centering
    \small
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Dataset} & \textbf{Train} & \textbf{Test} & \textbf{Total} \\
        \hline
        \multicolumn{4}{|c|}{\textbf{Twitter Dataset}} \\
        \hline
        Real News & 3,324 & 738 & 4,062 \\
        Fake News & 3,410 & 758 & 4,168 \\
        \hline
        \textbf{Total} & \textbf{6,734} & \textbf{1,496} & \textbf{8,230} \\
        \hline
    \end{tabular}
    \vspace{0.2cm}
    
    \footnotesize
    \textit{Note:} All posts include both text (avg. 23 BERT tokens) and images (224×224×3 RGB).
    \label{tab:dataset_summary}
\end{table}


\section{Proposed Methodology}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=15cm]{Chap02/Spot fake design.png}
    \caption{Architecture of the Proposed Multimodal Fake News Detection System.}
    \label{fig1}
\end{figure*}

\subsection{System Architecture Overview}
The proposed system follows a two-tower encoder architecture with distinct text and image pathways that converge into a unified multimodal representation. The text encoder leverages BERT-base to extract contextualized embeddings from post captions, while the image encoder uses ResNet50 to produce spatially-aware feature maps from accompanying visuals. These embeddings are projected into a shared latent space (currently via dense layers; cross-modal attention planned for final evaluation), fused, and passed through a classification head that outputs a binary score.

\subsection{Encoder Architectures}

\subsubsection{Text Encoder: BERT-base}
We choose BERT-base for textual data as it can capture the meaning and the context of the words quite accurately. Basically, the model looks at both sides of a word (bidirectional), which is very helpful for figuring out complicated language, the feeling, or the deceptive kind of the news that is frequently the case with fake news.

BERT-base consists of 12 layers, 768 hidden units, and around 110 million parameters. First, it fragments the text into tokens by WordPiece and then it adds three types of embeddings—token, position, and segment (here, zero). The [CLS] token after going through all the layers, gives a 768-dimensional representation of the sentence, which is further brought down to 32 dimensions to be compatible with the image features.

Since BERT is trained on a huge amount of text data such as Wikipedia before, it is very much aware of the general language patterns and it is also very effective in misinformation detection, especially in cases where the context and the tone are involved.

\subsubsection{Image Encoder: ResNet50}
We have decided to go with ResNet50 instead of VGG19 for images. ResNet50 is a deeper network but it is more efficient in a way that it uses residual (skip) connections, which facilitate the training and prevent the problem of vanishing gradients.

The structure of the network is a 7×7 convolution and a pooling layer, then four residual blocks. Each block has small 1×1 and 3×3 convolutions and adds the input to the output. At last, a Global Average Pooling (GAP) layer gives a 2048-dimensional feature vector.

Some reasons why ResNet50 works better are:
\begin{itemize}
    \item It is able to learn fine as well as high-level details.
    \item It has fewer parameters (≈25M vs 143M in VGG19).
    \item It is very good at generalizing even when the fake news datasets are small or varied.
    \item Pre-training on ImageNet makes it have strong visual knowledge for memes and screenshots.
\end{itemize}


\subsection{Contrastive Pre-training for Cross-Modal Alignment}
To address the semantic misalignment between independently trained text and image encoders, we implement contrastive learning as a pre-training stage before supervised fine-tuning. Traditional concatenation-based fusion operates on embeddings learned in isolation, which occupy incompatible latent regions and fail to capture cross-modal semantic correspondence effectively.

Our contrastive pre-training approach employs an InfoNCE (Noise Contrastive Estimation) loss function that learns to align text-image representations in a shared embedding space. For each training batch, the loss maximizes cosine similarity between matched text-image pairs (originating from the same social media post) while simultaneously minimizing similarity with randomly sampled negative pairs from other posts in the mini-batch. This process teaches the encoders to recognize semantic relationships and contradictions between modalities—a critical capability for detecting misinformation where text and images may deliberately contradict each other.

Mathematically, for a batch of $N$ text-image pairs, the InfoNCE loss is computed as:
\[
\mathcal{L}_{\text{contrastive}} = -\frac{1}{N}\sum_{i=1}^{N} \log \frac{\exp(\text{sim}(z_i^t, z_i^v)/\tau)}{\sum_{j=1}^{N}\exp(\text{sim}(z_i^t, z_j^v)/\tau)}
\]
where $z_i^t$ and $z_i^v$ are the normalized text and image embeddings, $\text{sim}(\cdot,\cdot)$ denotes cosine similarity, and $\tau$ is a temperature parameter (set to 0.07 in our experiments).

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/contrastive_learning.png}
    \caption{Contrastive Learning for Cross-Modal Alignment. The diagram illustrates how contrastive learning organizes the embedding space by moving similar items (anchor and positives) closer together while pushing dissimilar items (negatives) further apart, creating distinct clusters for different categories.}
    \label{fig:contrastive_learning}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/contrastive_usage_example.png}
    \caption{Contrastive Learning Usage Example. This figure demonstrates the practical application of contrastive learning where text and image pairs are encoded and aligned in a shared embedding space, enabling better cross-modal understanding.}
    \label{fig:contrastive_usage_example}
\end{figure}

\subsection{Cross-Modal Attention for Selective Fusion}
While contrastive pre-training establishes semantic alignment between modalities, simple concatenation still treats all features uniformly regardless of their relevance to a specific instance. To address this limitation, we implement cross-modal attention mechanisms that enable the model to dynamically weight modality contributions based on content characteristics.

The attention mechanism learns to selectively emphasize the most informative modality for each post. For example, when text is vague or generic (e.g., "Breaking news!" or "You won't believe this"), but the accompanying image contains clear visual evidence (such as a manipulated photo or inconsistent metadata), the attention mechanism up-weights visual features. Conversely, when the image is uninformative (e.g., a generic stock photo or landscape), the model focuses primarily on textual cues.

Our implementation uses multi-head cross-modal attention where text embeddings serve as queries ($Q$) and image embeddings provide keys ($K$) and values ($V$). The attention scores are computed as:
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
where $d_k$ is the dimension of the key vectors. This produces attention-weighted multimodal representations that capture fine-grained interactions between textual and visual information, significantly improving the model's ability to detect subtle inconsistencies indicative of misinformation.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/cross-attention.png}
    \caption{Cross-Modal Attention Mechanism. The architecture shows how visual and textual encoders feed into linear projection layers, followed by multi-head cross-modal attention that selectively fuses information from both modalities through Query-Key-Value transformations, producing cross-modal attention scores.}
    \label{fig:cross_attention}
\end{figure}

\subsection{Explainability Framework: Grad-CAM, SHAP, and Attention Visualization}
To ensure transparency and interpretability in our multimodal fake news detection system, we integrate three complementary explainability techniques that provide insights into the model's decision-making process at different levels: visual, textual, and cross-modal.

\subsubsection{Grad-CAM for Visual Explanation}
Grad-CAM (Gradient-weighted Class Activation Mapping) identifies and visualizes the most influential regions in input images that drive the model's predictions. By computing the gradient of the predicted class score with respect to the final convolutional feature maps, Grad-CAM generates a coarse localization map highlighting discriminative regions. The image is then overlaid with a color-coded heatmap that indicates which areas—such as faces, objects, text overlays, or manipulated regions—most strongly influenced the classification decision. This visualization helps verify whether the model focuses on semantically relevant visual cues or potentially spurious artifacts.

\subsubsection{SHAP for Textual Explanation}
SHAP (SHapley Additive exPlanations) provides token-level importance scores for textual input, revealing which words contribute most significantly to the prediction. Based on game-theoretic Shapley values, SHAP computes the marginal contribution of each token by systematically evaluating predictions across all possible token subsets. The resulting importance scores identify the most influential textual features—such as sensational phrases, emotional language, or contradictory statements—that drive the model's fake news classification. This enables fine-grained error analysis and helps identify potential biases or misleading patterns in the text encoder.

\subsubsection{Cross-Modal Attention Heatmaps}
Attention heatmaps visualize the learned interactions between textual and visual modalities, illustrating how the model associates specific words with corresponding image regions. By examining the attention weights from the cross-modal attention layer, we can identify which parts of the image align with particular textual tokens. For instance, a mention of "crowd" in the text should ideally attend to crowd regions in the image, while contradictions (e.g., "empty street" with crowded image) should exhibit misaligned attention patterns. These visualizations provide crucial insights into the cross-modal reasoning process and help validate whether the model effectively correlates information across both modalities or relies on superficial features.

Together, these three explainability techniques transform the model from a black-box classifier into an interpretable system that provides actionable insights for content moderators, fact-checkers, and end-users seeking to understand the rationale behind fake news predictions.



\section{Training Protocols and Results}

\subsection{Training Configuration}
The model was trained using the following hyperparameters:
\begin{itemize}
    \item \textbf{Optimizer:} Adam ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1\times10^{-8}$)
    \item \textbf{Learning rate:} $5\times10^{-4}$
    \item \textbf{Batch size:} 512 global (256 per GPU for multi-GPU training)
    \item \textbf{Epochs:} 20 (with early stopping on validation accuracy, patience=5)
    \item \textbf{Dropout:} 0.4 in MLP layers
    \item \textbf{Loss function:} Binary cross-entropy
\end{itemize}

\subsection{Dataset Description}
The Twitter fake news dataset was used with the provided train/test split. Images were filtered to retain only posts with available visuals, and text was preprocessed using the BERT tokenizer. The class distribution is approximately balanced between fake and real news posts.

\subsection{Model Comparison}
Table~\ref{tab:model_comparison} presents a comparison between the baseline VGG19-BERT model and the current ResNet50-BERT model.

\begin{table}[!ht]
    \caption{Performance Comparison: VGG19-BERT vs ResNet50-BERT.}
    \centering
    \small
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Text} & \textbf{Image} & \textbf{Fusion} & \textbf{Accuracy} & \textbf{F1} & \textbf{Training Time} \\
        \textbf{Variant} & \textbf{Encoder} & \textbf{Encoder} & & & & \textbf{(per epoch)} \\
        \hline
        VGG19-BERT & BERT & VGG19 & Concat & 0.77 & 0.76 & 2.5 min \\
        (Baseline) & & & & & & \\
        \hline
        ResNet50-BERT & BERT & ResNet50 & Concat & \textbf{0.79} & \textbf{0.78} & 2.8 min \\
        (Current) & & & & & & \\
        \hline
    \end{tabular}
    \label{tab:model_comparison}
\end{table}

The ResNet50-BERT model achieves a 2\% improvement in both accuracy and F1-score over the VGG19-BERT baseline, with only a marginal increase in training time (0.3 min/epoch). This demonstrates that ResNet50's residual connections and more efficient feature extraction lead to better performance in multimodal fake news detection.

\subsection{Training Performance Visualization}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/vgg19_graph.png}
    \caption{Training and validation accuracy/loss curves for VGG19-BERT baseline model. The validation loss remains relatively stable but shows fluctuations after epoch 6, indicating potential overfitting issues with the VGG19 encoder.}
    \label{fig:vgg19_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/resnet50_graph.png}
    \caption{Training and validation accuracy/loss curves for ResNet50-BERT model. The validation loss shows better convergence with fewer fluctuations compared to VGG19, and the training accuracy steadily increases to around 90\% while maintaining good generalization on validation data.}
    \label{fig:resnet50_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/complete_model.png}
    \caption{Training and validation accuracy/loss curves for the complete multimodal system integrating ResNet50-BERT encoders with contrastive pre-training, cross-modal attention, and explainability framework. The model demonstrates stable convergence with training accuracy reaching approximately 90\% while maintaining strong generalization on validation data. The smooth learning curves indicate effective cross-modal alignment through contrastive learning, and the consistent gap between training and validation metrics suggests the model avoids overfitting despite the sophisticated architecture.}
    \label{fig:complete_model_training}
\end{figure}

\subsection{Explainability Results: Visual Analysis}
To validate the interpretability of our multimodal system, we present qualitative explainability results from Grad-CAM and SHAP analyses on test set predictions. These visualizations demonstrate how the model reasons across both visual and textual modalities when classifying social media posts as fake or real news.

Figure~\ref{fig:gradcam_shap_results} illustrates representative examples of the explainability framework in action. The Grad-CAM heatmaps reveal that the model focuses on semantically relevant image regions—such as manipulated text overlays, inconsistent backgrounds, or suspicious visual artifacts—rather than spurious correlations. Simultaneously, SHAP token importance scores identify textual features like sensational language ("shocking", "unbelievable", "breaking"), emotional appeals, or contradictory statements that strongly influence the fake news classification.

These complementary explanations provide transparency into the model's decision-making process, enabling content moderators and fact-checkers to validate predictions, identify potential biases, and understand which multimodal cues drive the classification. The visualizations confirm that the model leverages meaningful cross-modal evidence rather than superficial patterns, establishing trust in its predictions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/gradcam_sample_0.png}
    \caption{Example visual explanations from Grad-CAM and SHAP for fake news detection. The left panel shows Grad-CAM heatmaps overlaid on input images, highlighting discriminative regions that influenced the prediction (e.g., manipulated text overlays, suspicious visual elements). The right panel presents SHAP value plots for the corresponding text, indicating key tokens (e.g., "shocking", "unbelievable") that contributed positively to the fake news classification. Red/warm colors indicate features supporting the fake label, while blue/cool colors represent features supporting the real label. These visualizations provide actionable insights into the model's reasoning process across both modalities.}
    \label{fig:gradcam_shap_results}
\end{figure}
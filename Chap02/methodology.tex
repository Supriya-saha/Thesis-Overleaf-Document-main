\chapter{Methodology for Multimodal Fake News Detection} \label{Methodology} 

\section{Motivation}
Existing systems for detecting fake news that use both text and images still face many challenges. They struggle to properly connect information between the two types of data and to explain how their decisions are made. Most of these systems simply combine the features from text and images. This method assumes both text and image contribute equally, but that’s not always true. Sometimes, the text is more important; other times, the image carries more meaning. Because of this, such systems can easily fail when one of them is missing or misleading.
The type of image model used also plays a big role in performance. Older models like VGG19 work well but have some drawbacks. They produce very large feature vectors and can easily overfit, especially when the dataset is small. They also suffer from problems like vanishing gradients during training. On the other hand, ResNet models solve these issues by using skip connections, which make training smoother. ResNet50 also gives more compact and meaningful image features. Studies have shown that ResNet-based models perform better when the data changes across domains and can generalize well even with less labeled data.
The semantic gap difference is another major problem between text and image models how they learn. Text models like BERT learn from language patterns, while image models learn from visual details like colors and objects. When these are trained separately, their outputs do not correspond, thus it is difficult for a model to figure out the relationship between the text and the image. Contrastive learning solves this problem by teaching the model to make the distances between the matching text-image pairs smaller and the mismatched ones larger. Thus the system can now detect agreements as well as contradictions between the image and the text.
At last, explainability is a vital factor in the establishment of trust in such systems. In the case where a model predicts an outcome without indicating the reason, it becomes quite difficult to trust or enhance it. Instruments such as Grad-CAM (for indicating the most relevant image regions), SHAP (for pointing out the most significant words), and attention maps (for demonstrating how text and image interact) help to disclose the model's reasoning.


Figure \ref{fig1} presents the proposed multimodal fake news detection architecture and Table \ref{tab:dataset_summary} summarizes the datasets used in this study.

\begin{table}[!ht]
    \caption{Summary of Datasets Used for Multimodal Fake News Detection.}
    \centering
    \small
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Dataset} & \textbf{Train} & \textbf{Test} & \textbf{Total} \\
        \hline
        \multicolumn{4}{|c|}{\textbf{Twitter Dataset}} \\
        \hline
        Real News & 3,324 & 738 & 4,062 \\
        Fake News & 3,410 & 758 & 4,168 \\
        \textbf{Subtotal} & \textbf{6,734} & \textbf{1,496} & \textbf{8,230} \\
        \hline
        \multicolumn{4}{|c|}{\textbf{Weibo Dataset}} \\
        \hline
        Real Events & 2,313 & 500 & 2,813 \\
        Fake Events & 2,351 & 508 & 2,859 \\
        \textbf{Subtotal} & \textbf{4,664} & \textbf{1,008} & \textbf{5,672} \\
        \hline
        \hline
        \textbf{Grand Total} & \textbf{11,398} & \textbf{2,504} & \textbf{13,902} \\
        \hline
    \end{tabular}
    \vspace{0.2cm}
    
    \footnotesize
    \textit{Note:} All posts include both text (avg. 23 BERT tokens) and images (224×224×3 RGB).
    \label{tab:dataset_summary}
\end{table}


\section{Proposed Methodology}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=15cm]{Chap02/Spot fake design.png}
    \caption{Architecture of the Proposed Multimodal Fake News Detection System.}
    \label{fig1}
\end{figure*}

\subsection{System Architecture Overview}
The proposed system follows a two-tower encoder architecture with distinct text and image pathways that converge into a unified multimodal representation. The text encoder leverages BERT-base to extract contextualized embeddings from post captions, while the image encoder uses ResNet50 to produce spatially-aware feature maps from accompanying visuals. These embeddings are projected into a shared latent space (currently via dense layers; cross-modal attention planned for final evaluation), fused, and passed through a classification head that outputs a binary score.

\subsection{Encoder Architectures}

\subsubsection{Text Encoder: BERT-base}
We choose BERT-base for textual data as it can capture the meaning and the context of the words quite accurately. Basically, the model looks at both sides of a word (bidirectional), which is very helpful for figuring out complicated language, the feeling, or the deceptive kind of the news that is frequently the case with fake news.

BERT-base consists of 12 layers, 768 hidden units, and around 110 million parameters. First, it fragments the text into tokens by WordPiece and then it adds three types of embeddings—token, position, and segment (here, zero). The [CLS] token after going through all the layers, gives a 768-dimensional representation of the sentence, which is further brought down to 32 dimensions to be compatible with the image features.

Since BERT is trained on a huge amount of text data such as Wikipedia before, it is very much aware of the general language patterns and it is also very effective in misinformation detection, especially in cases where the context and the tone are involved.

\subsubsection{Image Encoder: ResNet50}
We have decided to go with ResNet50 instead of VGG19 for images. ResNet50 is a deeper network but it is more efficient in a way that it uses residual (skip) connections, which facilitate the training and prevent the problem of vanishing gradients.

The structure of the network is a 7×7 convolution and a pooling layer, then four residual blocks. Each block has small 1×1 and 3×3 convolutions and adds the input to the output. At last, a Global Average Pooling (GAP) layer gives a 2048-dimensional feature vector.

Some reasons why ResNet50 works better are:
\begin{itemize}
    \item It is able to learn fine as well as high-level details.
    \item It has fewer parameters (≈25M vs 143M in VGG19).
    \item It is very good at generalizing even when the fake news datasets are small or varied.
    \item Pre-training on ImageNet makes it have strong visual knowledge for memes and screenshots.
\end{itemize}


\section{Training Protocols and Results}

\subsection{Training Configuration}
The model was trained using the following hyperparameters:
\begin{itemize}
    \item \textbf{Optimizer:} Adam ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1\times10^{-8}$)
    \item \textbf{Learning rate:} $5\times10^{-4}$
    \item \textbf{Batch size:} 512 global (256 per GPU for multi-GPU training)
    \item \textbf{Epochs:} 20 (with early stopping on validation accuracy, patience=5)
    \item \textbf{Dropout:} 0.4 in MLP layers
    \item \textbf{Loss function:} Binary cross-entropy
\end{itemize}

\subsection{Dataset Description}
The Twitter fake news dataset was used with the provided train/test split. Images were filtered to retain only posts with available visuals, and text was preprocessed using the BERT tokenizer. The class distribution is approximately balanced between fake and real news posts.

\subsection{Model Comparison}
Table~\ref{tab:model_comparison} presents a comparison between the baseline VGG19-BERT model and the current ResNet50-BERT model.

\begin{table}[!ht]
    \caption{Performance Comparison: VGG19-BERT vs ResNet50-BERT.}
    \centering
    \small
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Text} & \textbf{Image} & \textbf{Fusion} & \textbf{Accuracy} & \textbf{F1} & \textbf{Training Time} \\
        \textbf{Variant} & \textbf{Encoder} & \textbf{Encoder} & & & & \textbf{(per epoch)} \\
        \hline
        VGG19-BERT & BERT & VGG19 & Concat & 0.77 & 0.76 & 2.5 min \\
        (Baseline) & & & & & & \\
        \hline
        ResNet50-BERT & BERT & ResNet50 & Concat & \textbf{0.79} & \textbf{0.78} & 2.8 min \\
        (Current) & & & & & & \\
        \hline
    \end{tabular}
    \label{tab:model_comparison}
\end{table}

The ResNet50-BERT model achieves a 2\% improvement in both accuracy and F1-score over the VGG19-BERT baseline, with only a marginal increase in training time (0.3 min/epoch). This demonstrates that ResNet50's residual connections and more efficient feature extraction lead to better performance in multimodal fake news detection.

\subsection{Training Performance Visualization}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/vgg19_graph.png}
    \caption{Training and validation accuracy/loss curves for VGG19-BERT baseline model. The validation loss remains relatively stable but shows fluctuations after epoch 6, indicating potential overfitting issues with the VGG19 encoder.}
    \label{fig:vgg19_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/resnet50_graph.png}
    \caption{Training and validation accuracy/loss curves for ResNet50-BERT model. The validation loss shows better convergence with fewer fluctuations compared to VGG19, and the training accuracy steadily increases to around 90\% while maintaining good generalization on validation data.}
    \label{fig:resnet50_training}
\end{figure}
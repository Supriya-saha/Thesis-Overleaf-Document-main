\chapter{Methodology for Multimodal Fake News Detection} \label{Methodology} 

\section{Motivation}
Existing systems for detecting fake news that use both text and images still face many challenges. They struggle to properly connect information between the two types of data and to explain how their decisions are made. Most of these systems simply combine the features from text and images. This method assumes both text and image contribute equally, but that’s not always true. Sometimes, the text is more important; other times, the image carries more meaning. Because of this, such systems can easily fail when one of them is missing or misleading.
The type of image model used also plays a big role in performance. Older models like VGG19 work well but have some drawbacks. They produce very large feature vectors and can easily overfit, especially when the dataset is small. They also suffer from problems like vanishing gradients during training. On the other hand, ResNet models solve these issues by using skip connections, which make training smoother. ResNet50 also gives more compact and meaningful image features. Studies have shown that ResNet-based models perform better when the data changes across domains and can generalize well even with less labeled data.
The semantic gap difference is another major problem between text and image models how they learn. Text models like BERT learn from language patterns, while image models learn from visual details like colors and objects. When these are trained separately, their outputs do not correspond, thus it is difficult for a model to figure out the relationship between the text and the image. Contrastive learning solves this problem by teaching the model to make the distances between the matching text-image pairs smaller and the mismatched ones larger. Thus the system can now detect agreements as well as contradictions between the image and the text.
At last, explainability is a vital factor in the establishment of trust in such systems. In the case where a model predicts an outcome without indicating the reason, it becomes quite difficult to trust or enhance it. Instruments such as Grad-CAM (for indicating the most relevant image regions), SHAP (for pointing out the most significant words), and attention maps (for demonstrating how text and image interact) help to disclose the model's reasoning.


Figure \ref{fig1} presents the proposed multimodal fake news detection architecture and Table \ref{tab:dataset_summary} summarizes the datasets used in this study.

\begin{table}[!ht]
    \caption{Summary of Datasets Used for Multimodal Fake News Detection.}
    \centering
    \small
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Dataset} & \textbf{Train} & \textbf{Test} & \textbf{Total} \\
        \hline
        \multicolumn{4}{|c|}{\textbf{Twitter Dataset}} \\
        \hline
        Real News & 3,324 & 738 & 4,062 \\
        Fake News & 3,410 & 758 & 4,168 \\
        \hline
        \textbf{Total} & \textbf{6,734} & \textbf{1,496} & \textbf{8,230} \\
        \hline
    \end{tabular}
    \vspace{0.2cm}
    
    \footnotesize
    \textit{Note:} All posts include both text (avg. 23 BERT tokens) and images (224×224×3 RGB).
    \label{tab:dataset_summary}
\end{table}


\section{Proposed Methodology}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=15cm]{Chap02/spotfake_arch.png}
    \caption{Architecture of the Proposed Multimodal Fake News Detection System.}
    \label{fig1}
\end{figure*}

\subsection{System Architecture Overview}
The proposed system follows a two-tower encoder architecture with distinct text and image pathways that converge into a unified multimodal representation. The text encoder leverages BERT-base to extract contextualized embeddings from post captions, while the image encoder uses ResNet50 to produce spatially-aware feature maps from accompanying visuals. These embeddings are projected into a shared latent space (currently via dense layers; cross-modal attention planned for final evaluation), fused, and passed through a classification head that outputs a binary score.

\subsection{Encoder Architectures}

\subsubsection{Text Encoder: BERT-base}
We choose BERT-base for textual data as it can capture the meaning and the context of the words quite accurately. Basically, the model looks at both sides of a word (bidirectional), which is very helpful for figuring out complicated language, the feeling, or the deceptive kind of the news that is frequently the case with fake news.

BERT-base consists of 12 layers, 768 hidden units, and around 110 million parameters. First, it fragments the text into tokens by WordPiece and then it adds three types of embeddings—token, position, and segment (here, zero). The [CLS] token after going through all the layers, gives a 768-dimensional representation of the sentence, which is further brought down to 32 dimensions to be compatible with the image features.

Since BERT is trained on a huge amount of text data such as Wikipedia before, it is very much aware of the general language patterns and it is also very effective in misinformation detection, especially in cases where the context and the tone are involved.

\subsubsection{Image Encoder: ResNet50}
We have decided to go with ResNet50 instead of VGG19 for images. ResNet50 is a deeper network but it is more efficient in a way that it uses residual (skip) connections, which facilitate the training and prevent the problem of vanishing gradients.

The structure of the network is a 7×7 convolution and a pooling layer, then four residual blocks. Each block has small 1×1 and 3×3 convolutions and adds the input to the output. At last, a Global Average Pooling (GAP) layer gives a 2048-dimensional feature vector.

Some reasons why ResNet50 works better are:
\begin{itemize}
    \item It is able to learn fine as well as high-level details.
    \item It has fewer parameters (≈25M vs 143M in VGG19).
    \item It is very good at generalizing even when the fake news datasets are small or varied.
    \item Pre-training on ImageNet makes it have strong visual knowledge for memes and screenshots.
\end{itemize}


\subsection{Contrastive Pre-training for Cross-Modal Alignment}
To address the semantic misalignment between independently trained text and image encoders, we implement contrastive learning as a pre-training stage before supervised fine-tuning. Traditional concatenation-based fusion operates on embeddings learned in isolation, which occupy incompatible latent regions and fail to capture cross-modal semantic correspondence effectively.

Our contrastive pre-training approach employs an InfoNCE (Noise Contrastive Estimation) loss function that learns to align text-image representations in a shared embedding space. For each training batch, the loss maximizes cosine similarity between matched text-image pairs (originating from the same social media post) while simultaneously minimizing similarity with randomly sampled negative pairs from other posts in the mini-batch. This process teaches the encoders to recognize semantic relationships and contradictions between modalities—a critical capability for detecting misinformation where text and images may deliberately contradict each other.

Mathematically, for a batch of $N$ text-image pairs, the InfoNCE loss~\cite{li2019visualbert} is computed as:
\[
\mathcal{L}_{\text{contrastive}} = -\frac{1}{N}\sum_{i=1}^{N} \log \frac{\exp(\text{sim}(z_i^t, z_i^v)/\tau)}{\sum_{j=1}^{N}\exp(\text{sim}(z_i^t, z_j^v)/\tau)}
\]
where $z_i^t$ and $z_i^v$ are the normalized text and image embeddings, $\text{sim}(\cdot,\cdot)$ denotes cosine similarity, and $\tau$ is a temperature parameter (set to 0.07 in our experiments).

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/contrastive_learning.png}
    \caption{Contrastive Learning for Cross-Modal Alignment. The figure explains how contrastive learning decorates the embedding space by bringing close similar items (anchor and positives) and at the same time pushing away dissimilar items (negatives), thus creating well-separated clusters for different categories.}
    \label{fig:contrastive_learning}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/contrastive_usage_example.png}
    \caption{Contrastive Learning Usage Example. The illustration depicts an instance of contrastive learning implementation in which both text and image are encoded and aligned in a common embedding space which leads to the improved cross-modal understanding.}
    \label{fig:contrastive_usage_example}
\end{figure}

\subsection{Cross-Modal Attention for Selective Fusion}
Although contrastive pre-training helps to closely match the semantics between different modalities, simply concatenating the features still treats all features as if they were equally important. In order to solve this problem, we have introduced cross-modal attention mechanisms in our model which allow it to adjust the weights of modality contributions depending on the content characteristics.
The attention mechanism selects the modality that is most informative for a given post and tries to focus on it. For instance, if a text is unclear or very general (e.g., "Breaking news!" or "You won't believe this"), but an image shows some obvious visual facts (like a manipulated photo or inconsistent metadata), the attention mechanism will be increasing the contribution of the visual features. On the other hand, if an image is not helpful at all (e.g., a generic stock photo or a landscape), the model will concentrate mainly on textual cues.

We have employed multi-head cross-modal attention in our system, where text embeddings act as queries ($Q$), and image embeddings are keys ($K$) and values ($V$). The attention scores are computed as:
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
where $d_k$ denotes the dimension of the key vectors. This results in attention-weighted multimodal representations that can even capture very subtle interactions between textual and visual information, thus greatly enhancing the capability of the model in detecting slight inconsistencies that are usually the case with misinformation.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/cross-attention.png}
    \caption{Cross-Modal Attention Mechanism. The different layers represent how the visual and textual features are first passed to the linear projection layers and then to the multi-head cross-modal attention which through Query-Key-Value transformations selectively integrates information from both the modalities thus generating cross-modal attention scores.}
    \label{fig:cross_attention}
\end{figure}

\subsection{Explainability Framework: Grad-CAM, SHAP, and Attention Visualization}
In order to reveal the working mechanism of our multimodal fake news detection system and to make it interpretable to people, we employ three different, but related, explainability techniques that illustrate the model's decision process visually, textually, and cross-modally.

\subsubsection{Grad-CAM for Visual Explanation}
Grad-CAM (Gradient-weighted Class Activation Mapping) helps to find and visualize the areas in the input images that have the greatest influence on the model output. Grad-CAM creates a localization map of the most relevant areas by calculating the gradient of the target class score with the last convolutional features. The map is then color-coded and overlaid on the original image to show those regions - for example, faces, objects, or manipulated areas - that contributed most to the classification decision. This method of presentation serves the purpose of confirming if the visual cues the model relies on are semantically correct or if they are just some random artifacts.

\subsubsection{SHAP for Textual Explanation}
SHAP (SHapley Additive exPlanations) gives local importance scores of tokens for the text input, thus uncovering the words that influence the prediction the most. SHAP relies on game-theoretic Shapley value principles and it determines the contribution of each token by evaluating the prediction for every possible token subset. The computed importance scores pinpoint the most influential textual features—say for instance sensational phrases, emotional words, or contradictory statements—that in turn determine the fake news classification done by the model. This facilitates granular error analysis and also assists in recognizing the bias and the misleading patterns in the text encoder which are caused by the use of specific words.

\subsubsection{Cross-Modal Attention Heatmaps}
Attention heatmaps represent the mutual referential relations between language and vision models that the system has learned. They demonstrate how the model links certain words with the relevant image segments. The weights of the cross-modal attention can be used to locate those parts in the picture that correspond to certain text tokens. To give an example, the word "crowd" in the text should point towards the crowd in the photo whereas a case of contradiction (e.g., "empty street" and a photo of a crowd) should show that the attention is not aligned. These are very important findings from the cross-modal reasoning process that the model uses and help verify whether the model efficiently correlates information across two different modalities or depends on shallow features.
Together, these three explainability techniques transform the model from a black-box classifier into an interpretable system that provides actionable insights for content moderators, fact-checkers, and end-users seeking to understand the rationale behind fake news predictions.

\section{Training Protocols and Results}

\subsection{Training Configuration}
The model was trained using the following hyperparameters:
\begin{itemize}
    \item \textbf{Optimizer:} Adam ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1\times10^{-8}$)
    \item \textbf{Learning rate:} $5\times10^{-4}$
    \item \textbf{Batch size:} 512 global (256 per GPU for multi-GPU training)
    \item \textbf{Epochs:} 20 (with early stopping on validation accuracy, patience=5)
    \item \textbf{Dropout:} 0.4 in MLP layers
    \item \textbf{Loss function:} Binary cross-entropy
\end{itemize}

\subsection{Dataset Description}
In the Twitter fake news dataset, the train/test split as given was utilized. To focus only on posts with visuals, images were filtered, and text was preprocessed using the BERT tokenizer. The class distribution for the dataset is around the same number of fake and real news posts.

\subsection{Model Comparison}
Table~\ref{tab:model_comparison} compares the baseline VGG19-BERT model with the present ResNet50-BERT model.

\begin{table}[!ht]
    \caption{Performance Comparison: VGG19-BERT vs ResNet50-BERT.}
    \centering
    \small
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Text} & \textbf{Image} & \textbf{Fusion} & \textbf{Accuracy} & \textbf{F1} & \textbf{Training Time} \\
        \textbf{Variant} & \textbf{Encoder} & \textbf{Encoder} & & & & \textbf{(per epoch)} \\
        \hline
        VGG19-BERT & BERT & VGG19 & Concat & 0.77 & 0.76 & 2.5 min \\
        (Baseline) & & & & & & \\
        \hline
        ResNet50-BERT & BERT & ResNet50 & Cross Model & \textbf{0.79} & \textbf{0.77} & 3.1 min \\
        (Current) & & & & & & \\
        \hline
    \end{tabular}
    \label{tab:model_comparison}
\end{table}

ResNet50-BERT model is able to record a 2\% increment in both accuracy and F1-score as compared to the VGG19-BERT baseline with a very slight increase in the training time (0.3 min/epoch). So, it shows that residual connections of ResNet50 and its more efficient feature extraction are the reasons why it performs better in multimodal fake news detection.

\subsection{Training Performance Visualization}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/vgg19_graph.png}
    \caption{Training and validation accuracy/loss curves for the VGG19-BERT baseline model. Validation loss stays quite steady but, after epoch 6, it shows ups and downs, which is a sign of possible overfitting problems with the VGG19 encoder.}
    \label{fig:vgg19_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/resnet50_graph.png}
    \caption{Training and validation accuracy/loss curves for the ResNet50-BERT model. Validation loss converges better with fewer ups and downs as compared to VGG19, and training accuracy keeps on going up to about 90\% while at the same time good performance on validation data is retained.}
    \label{fig:resnet50_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/complete_model.png}
    \caption{Training and validation accuracy/loss curves for the full multimodal system combining ResNet50-BERT encoders with contrastive pre-training, cross-modal attention, and explainability framework. The model shows stable convergence with training accuracy going up to about 90\% while at the same time good performance on validation data is retained. The smooth learning curves point to successful cross-modal alignment via contrastive learning, and the stable difference between training and validation metrics indicates that the model is not overfitting even though the architecture is complex.}
    \label{fig:complete_model_training}
\end{figure}

\subsection{Explainability Results: Visual Analysis}
As a proof of the interpretability of our multimodal system, we show here qualitative explainability results obtained by Grad-CAM and SHAP on test set predictions. These visualizations explain how the model makes use of both visual and textual modalities to decide whether the social media posts are fake or real news.

Figure \ref{fig:gradcam_shap_results} shows the selected instances of the explainability framework at work. The Grad-CAM heatmaps unveil that the model attends to the image areas that are most relevant from the semantic point of view - for example, in the case of manipulated text overlays, inconsistent backgrounds, or suspicious visual artifacts - instead of spurious correlations. At the same time, SHAP token importance scores point to the textual features such as the use of sensational language ("shocking", "unbelievable", "breaking"), emotional appeals, or contradictory statements that most heavily weigh towards the fake news class.

These two sets of explanations pave the way for understanding the inner workings of the model decision process, thus they can be very helpful to content moderators and fact-checkers, who can use them to confirm the predictions, identify the sources of any biases, and figure out which multimodal cues led to the classification. The visualizations attest that the model is utilizing substantial cross-modal evidence rather than superficial patterns, thereby putting forward the idea of trust in its predictions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Chap02/image.png}
    \caption{Illustrative visual explanations for fake news detection from Grad-CAM and SHAP. The left column contains input images over which Grad-CAM heatmaps are superimposed, thus showing those parts of the image that gave the greatest support to the prediction (e.g., text overlays that have been manipulated, areas of the image that contain a suspicious visual element). The right panel has SHAP value plots for the corresponding text, wherein the main tokens (e.g., "shocking", "unbelievable") are shown which helped the fake news classification to a great extent. }
    \label{fig:gradcam_shap_results}
\end{figure}
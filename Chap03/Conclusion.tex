\chapter{Conclusion and Future Work}

\section{Summary}
This report presents the design and partial implementation of an enhanced multimodal fake news detection system that integrates advanced visionâ€“language techniques to address fundamental limitations in existing pipelines. The motivation stems from the inadequacies of late fusion strategies, weak cross-modal alignment, and opacity in decision-making. Preliminary architecture choices have been validated through ablation planning: comparing ResNet50 against the VGG19 baseline will quantify gains in accuracy.

\section{Future Work}
While the current system successfully integrates contrastive learning, cross-modal attention, and explainability techniques, several promising directions remain for future enhancement:

\begin{itemize}
    \item \textbf{Multi-Dataset Generalization and Cross-Platform Evaluation:} Extend the model's training and evaluation to diverse social media platforms (Facebook, Instagram, WhatsApp) and datasets beyond Twitter, including Weibo, FakeNewsNet, and Fakeddit. This would validate the system's generalization capability across different linguistic contexts, visual styles, and misinformation patterns. Additionally, incorporating multilingual datasets would enable fake news detection in non-English languages, significantly broadening the model's real-world applicability.
    
    \item \textbf{Hyperparameter Optimization and Architecture Search:} Conduct systematic hyperparameter tuning using techniques such as Bayesian optimization or grid search to identify optimal configurations for learning rate schedules, attention head counts, contrastive loss temperature ($\tau$), and dropout rates. Furthermore, exploring neural architecture search (NAS) could automatically discover more efficient encoder architectures or attention mechanisms that balance accuracy with computational efficiency, potentially reducing inference time for production deployment.
\end{itemize}

\section{Limitations and Challenges}
Even if the proposed changes cover most of the issues, a few limitations remain:
\begin{itemize}
    \item \textbf{Dataset Constraints:} For the training of the model, only Twitter posts were used as data. The model's capability to work with other platforms (Facebook, WhatsApp), different languages, or various multimedia formats (video, audio) has not been verified.
    \item \textbf{Computational Cost:} The training of the model is getting slower due to contrastive pre-training and attention mechanisms.
    \item \textbf{Interpretability Gaps:} Both Grad-CAM and SHAP methods indicate correlations but not causations. Attention is a necessary condition, but it is not sufficient for the explanation (high attention $\neq$ causal relevance).
    \item \textbf{Class Imbalance:} In the case of a significantly biased real/fake distribution, the accuracy may provide a deceptive signal.
    \item \textbf{Adversarial Robustness:} The model has not been tested for adversarial perturbations (e.g., adding imperceptible noise to images or paraphrasing text). Determining robustness through adversarial attacks (FGSM, PGD) should be the following step.
\end{itemize}

\section{Concluding Remarks}
This report establishes the foundation for a principled, interpretable, and scalable multimodal deepfake news detection. The integration of residual learning (ResNet50), contrastive alignment (CLIP-style pre-training), selective fusion (cross-modal attention), and transparent explanations (Grad-CAM, SHAP) addresses critical gaps in existing approaches. While ResNet50 integration lays the groundwork, contrastive pre-training, attention, and explainability, will deliver the promised improvements in accuracy and robustness.

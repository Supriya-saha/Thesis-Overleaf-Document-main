\chapter{Conclusion and Future Work}

\section{Summary}
This report presents the design and partial implementation of an enhanced multimodal fake news detection system that integrates advanced visionâ€“language techniques to address fundamental limitations in existing pipelines. The motivation stems from the inadequacies of late fusion strategies, weak cross-modal alignment, and opacity in decision-making. Preliminary architecture choices have been validated through ablation planning: comparing ResNet50 against the VGG19 baseline will quantify gains in accuracy.

\section{Future Work}

\subsection{Contrastive Pre-training for Cross-Modal Alignment}
Addition of Contrastive learning to pre-train text and image encoders jointly before supervised fine-tuning. Current concatenation-based fusion operates on embeddings learned independently, which occupy incompatible latent regions and fail to model semantic correspondence. Contrastive learning addresses this by optimizing an InfoNCE loss that maximizes cosine similarity for matched text-image pairs (from the same post) while minimizing similarity for randomly sampled negative pairs within each mini-batch.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/contrastive_learning.png}
    \caption{Contrastive Learning for Cross-Modal Alignment. The diagram illustrates how contrastive learning organizes the embedding space by moving similar items (anchor and positives) closer together while pushing dissimilar items (negatives) further apart, creating distinct clusters for different categories.}
    \label{fig:contrastive_learning}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/contrastive_usage_example.png}
    \caption{Contrastive Learning Usage Example. This figure demonstrates the practical application of contrastive learning where text and image pairs are encoded and aligned in a shared embedding space, enabling better cross-modal understanding.}
    \label{fig:contrastive_usage_example}
\end{figure}

\subsection{Cross-Modal Attention for Selective Fusion}
While contrastive learning aligns modalities, concatenation still treats them uniformly at the instance level. Cross-modal attention resolves this by enabling the model to dynamically weight modality contributions based on content. For instance, if text is vague ("Breaking news!") but the image shows clear evidence (manipulated photo), attention up-weights visual features; conversely, if the image is uninformative (generic landscape), attention focuses on text.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{Chap03/cross-attention.png}
    \caption{Cross-Modal Attention Mechanism. The architecture shows how visual and textual encoders feed into linear projection layers, followed by multi-head cross-modal attention that selectively fuses information from both modalities through Query-Key-Value transformations, producing cross-modal attention scores.}
    \label{fig:cross_attention}
\end{figure}

\subsection{Explainability Framework: Grad-CAM and SHAP}
To render the model less of a black box and more interpretable, three explainability measures Grad-CAM, SHAP, and Attention Heatmap were introduced. 

Grad-CAM (Gradient-weighted Class Activation Mapping) supports the identification and visualization of the key input image areas, which in turn had the most significant impact on the model's decision. With this technique, the image becomes overlaid with a heatmap marking the regions that predominantly drive the prediction, thus providing an indication of whether the model is focusing on the relevant visual cues such as faces, objects, or manipulated areas.

SHAP (Shapley Additive Explanations) is a technology that highlights the words that have the highest contribution to the output. It comes to such a conclusion by determining the prediction each word has, thus giving the importance scores that point out the most influential textual features.

Attention heatmaps are a different modality used to establish the interaction between different modalities. Such means illustrate how a model associates' language and images by pointing out the visual parts that correspond to certain words in the text. This, in turn, assists in comprehending the cross-modal reasoning process and verifying if the model is efficiently correlating information across both modalities.

\section{Limitations and Challenges}
Even if the proposed changes cover most of the issues, a few limitations remain:
\begin{itemize}
    \item \textbf{Dataset Constraints:} For the training of the model, only Twitter posts were used as data. The model's capability to work with other platforms (Facebook, WhatsApp), different languages, or various multimedia formats (video, audio) has not been verified.
    \item \textbf{Computational Cost:} The training of the model is getting slower due to contrastive pre-training and attention mechanisms.
    \item \textbf{Interpretability Gaps:} Both Grad-CAM and SHAP methods indicate correlations but not causations. Attention is a necessary condition, but it is not sufficient for the explanation (high attention $\neq$ causal relevance).
    \item \textbf{Class Imbalance:} In the case of a significantly biased real/fake distribution, the accuracy may provide a deceptive signal.
    \item \textbf{Adversarial Robustness:} The model has not been tested for adversarial perturbations (e.g., adding imperceptible noise to images or paraphrasing text). Determining robustness through adversarial attacks (FGSM, PGD) should be the following step.
\end{itemize}

\section{Concluding Remarks}
This report establishes the foundation for a principled, interpretable, and scalable multimodal deepfake news detection. The integration of residual learning (ResNet50), contrastive alignment (CLIP-style pre-training), selective fusion (cross-modal attention), and transparent explanations (Grad-CAM, SHAP) addresses critical gaps in existing approaches. While ResNet50 integration lays the groundwork, contrastive pre-training, attention, and explainability, will deliver the promised improvements in accuracy and robustness.
